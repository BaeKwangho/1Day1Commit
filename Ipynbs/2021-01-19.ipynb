{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import generator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,LSTM,Bidirectional,Embedding,Conv2D,BatchNormalization,Activation,Masking,LeakyReLU\n",
    "from tensorflow.keras.activations import tanh,relu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Model as models\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder as one_hot\n",
    "\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from IPython.display import display\n",
    "from time import sleep\n",
    "\n",
    "print('Test 1')\n",
    "dh = display('Test2',display_id=True)\n",
    "sleep(1)\n",
    "dh.update('Test3')\n",
    "\n",
    "files , json,json_word = generator.gen()\n",
    "maxi = 1021\n",
    "\n",
    "def expdim(x,axis):\n",
    "    return tf.expand_dims(x,axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2020\n",
      "2040\n",
      "2060\n",
      "2080\n",
      "2100\n",
      "2120\n",
      "2140\n",
      "2160\n",
      "2180\n",
      "2200\n",
      "2220\n",
      "2240\n",
      "2260\n",
      "2280\n",
      "2300\n",
      "2320\n",
      "2340\n",
      "2360\n",
      "2380\n",
      "2400\n",
      "2420\n",
      "2440\n",
      "2460\n",
      "2480\n",
      "2500\n",
      "2520\n",
      "2540\n",
      "2560\n",
      "2580\n",
      "2600\n",
      "2620\n",
      "2640\n",
      "2660\n",
      "2680\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#공식 구해야함\n",
    "'''\n",
    "maxi = 0\n",
    "for k,i in enumerate(files):\n",
    "    wav,_ = librosa.load(files[i]['file_path'],sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(wav,n_mfcc=40)\n",
    "    if mfcc.shape[1] > maxi:\n",
    "        maxi = mfcc.shape[1]\n",
    "    if k%200==0:\n",
    "        print(k)\n",
    "'''\n",
    "one_hot_json = []\n",
    "for i,k in enumerate(json):\n",
    "    one_hot_json.append([json[k][0]])\n",
    "one_hot_enc=one_hot()\n",
    "one_hot_enc.fit(one_hot_json)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(10e-3)\n",
    "maxi = 1021\n",
    "sos_emb = emb_layer(1)\n",
    "\n",
    "emb_results = [None]*len(files)\n",
    "mfccs_temp = []\n",
    "#전체 파일 생성\n",
    "files_word_num = []\n",
    "\n",
    "max_len_txt = 0\n",
    "for k,i in enumerate(files):\n",
    "    if k % 20 == 0:\n",
    "        print(k)\n",
    "    file_word_num = [ json[j][0] for j in files[i]['file_txt']]\n",
    "    files_word_num.append(file_word_num)\n",
    "    wav,_ = librosa.load(files[i]['file_path'],sr=16000)\n",
    "    \n",
    "    if max_len_txt < len(file_word_num):\n",
    "        max_len_txt = len(file_word_num)\n",
    "        \n",
    "    emb_results[k] = emb_result\n",
    "    mfcc = np.expand_dims(\n",
    "                np.expand_dims(\n",
    "                    librosa.feature.mfcc(wav,n_mfcc=40),axis=-1),axis=0)\n",
    "\n",
    "    tak = np.zeros((1,40,maxi-mfcc.shape[-2],1),dtype=np.float32)\n",
    "    mfcc = np.concatenate([mfcc,tak],axis=2)\n",
    "    mfccs_temp.append(mfcc)\n",
    "mfccs = np.concatenate(mfccs_temp,axis=0)\n",
    "\n",
    "embs = []\n",
    "for j in files_word_num:\n",
    "    pad = np.zeros([max_len_txt-len(j)])\n",
    "    embs.append(np.expand_dims(np.concatenate([j,pad],axis=0),axis=0))\n",
    "embs = np.concatenate(embs,axis=0)\n",
    "embs.astype(int)\n",
    "\n",
    "train_mfccs = mfccs[:-270]    \n",
    "test_mfccs = mfccs[-270:]\n",
    "\n",
    "train_embs = embs[:-270]    \n",
    "test_embs = embs[-270:]\n",
    "\n",
    "train_len = files_word_num[:-270]\n",
    "test_len = files_word_num[-270:]\n",
    "\n",
    "json_emb = tf.cast([ json[i][0] for i in json],tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2703, 94])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def emb_to_word(target):\n",
    "    cosine_result = cosine_similarity(emb_layer(json_emb),target)\n",
    "    result = []\n",
    "    for i in range(cosine_result.shape[1]):\n",
    "        table = cosine_result[:,i].tolist()\n",
    "        result.append(json_word[table.index(max(table))])\n",
    "    return result\n",
    "\n",
    "def hard_tanh(x):\n",
    "    return tf.minimum(20.0, tf.maximum(0.0, x)) \n",
    "    \n",
    "get_custom_objects().update({'hard_tanh':Activation(hard_tanh)})\n",
    "    \n",
    "class encoder(Model):\n",
    "    def conv_init(self):\n",
    "        conv = Sequential([\n",
    "            Conv2D(32,(21,81),strides=(2,2),padding='valid',input_shape=(40,1021,1)),\n",
    "            BatchNormalization(-1),\n",
    "            Activation(hard_tanh),\n",
    "            Conv2D(32,(10,41),strides=(1,2),padding='valid'),\n",
    "            BatchNormalization(-1),\n",
    "            Activation(hard_tanh),\n",
    "        ])\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        #self.conv = self.conv_init()\n",
    "        self.rnn = Sequential([\n",
    "            Masking(mask_value=0.0),\n",
    "            LSTM(256,input_shape=(None,40)),\n",
    "        ])\n",
    "        \n",
    "    def call(self,audio):\n",
    "        audio = tf.transpose(tf.squeeze(audio,3),[0,2,1])\n",
    "        #x = self.conv(audio)\n",
    "        #x = self.rnn(tf.squeeze(x,1))\n",
    "        x = self.rnn(audio)\n",
    "        return x\n",
    "        \n",
    "class seq2(Model):\n",
    "    def __init__(self, enc, dec):\n",
    "        super(seq2,self).__init__()\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        \n",
    "    def call(self,audio, txt_emb, txt_len,force_teaching=False):\n",
    "        enc_out = self.enc(audio)\n",
    "        dec_out = self.dec(enc_out,txt_emb, txt_len,force_teaching)\n",
    "        \n",
    "        return dec_out\n",
    "    \n",
    "    def predict(self, audio, sos_emb):\n",
    "        \n",
    "        enc_out = self.enc(audio)\n",
    "        enc_out = tf.expand_dims(enc_out,axis=0)\n",
    "        \n",
    "        sos_emb = tf.expand_dims(tf.expand_dims(sos_emb,axis=0),axis=0)\n",
    "        \n",
    "        predict_emb = tf.concat([sos_emb,enc_out],axis=2)\n",
    "        \n",
    "        #init\n",
    "        #predict_emb = self.dec.mask(predict_emb)\n",
    "        output, h , c = self.dec.rnn(predict_emb)\n",
    "        cur_word = tf.expand_dims(tf.expand_dims(output[0,-1,:],axis=0),axis=0)\n",
    "        output_dim = output\n",
    "        \n",
    "        hidden = [h,c]\n",
    "        stop_sign = False\n",
    "        result = [sos_emb]\n",
    "        \n",
    "        k=0\n",
    "        while not stop_sign:\n",
    "            \n",
    "            '''if k == 3:\n",
    "                break'''\n",
    "            enc_outs = tf.tile(enc_out,[1,output_dim.shape[1],1])\n",
    "            predict_emb = tf.concat([output_dim,enc_outs],axis=2)\n",
    "            \n",
    "            #predict_emb = tf.concat([cur_word,enc_out],axis=2)\n",
    "            \n",
    "            #predict_emb = self.dec.mask(tf.concat([cur_word,enc_out],axis=2))\n",
    "            \n",
    "            print(predict_emb.shape)\n",
    "            output , h ,c = self.dec.rnn(\n",
    "                predict_emb,\n",
    "                initial_state = hidden\n",
    "            )\n",
    "            hidden = [h,c]\n",
    "            \n",
    "            pre_word = cur_word\n",
    "            cur_word = tf.expand_dims(tf.expand_dims(output[0,-1,:],axis=0),axis=0)\n",
    "            \n",
    "            #print(pre_word,cur_word)\n",
    "            #print(cosine_similarity(tf.squeeze(pre_word,0),tf.squeeze(cur_word,0)))\n",
    "            \n",
    "            output_dim = tf.concat([output_dim,cur_word],axis=1)\n",
    "            print(output_dim.shape,output.shape)\n",
    "            \n",
    "            result.append(cur_word)\n",
    "            \n",
    "            if len(result) > 93:\n",
    "                stop_sign = True\n",
    "            k+=1\n",
    "        \n",
    "        return result\n",
    "        \n",
    "        \n",
    "\n",
    "class decoder(Model):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        \n",
    "        self.emb = Embedding(len(json),512)\n",
    "        self.mask = Masking(mask_value=0.0)\n",
    "        self.rnn = LSTM(512,input_shape=(None,768),return_sequences=True,return_state=True)\n",
    "        \n",
    "    def call(self,enc_out,embs,txt_len,force_teaching=False):\n",
    "        \n",
    "        embs = self.emb(embs)\n",
    "        re_all=[]\n",
    "        padded = []\n",
    "        for i,k in enumerate(txt_len):\n",
    "            tiled = tf.tile(tf.expand_dims(tf.expand_dims(enc_out[i],axis=0),axis=0),[1,len(k),1])\n",
    "            pad = tf.convert_to_tensor(np.zeros((1,94-len(k),256)),tf.float32)\n",
    "            tiled = tf.concat([tiled,pad],axis=1)\n",
    "            padded.append(tiled)\n",
    "        padded = tf.concat(padded,axis=0)\n",
    "        \n",
    "        if force_teaching:\n",
    "            \n",
    "            cats = tf.concat([embs,padded],axis=2)\n",
    "            cat = self.mask(cats[:,0,:])\n",
    "            result,h,c = self.rnn(tf.expand_dims(cat,axis=1))\n",
    "            hidden = [h,c]\n",
    "            re_all.append(result)\n",
    "            \n",
    "        else:\n",
    "            inputs = tf.tile(tf.expand_dims(tf.expand_dims(sos_emb,axis=0),axis=0),[enc_out.shape[0],94,1])\n",
    "            cats = tf.concat([inputs,padded],axis=2)\n",
    "            \n",
    "            cat = self.mask(cats[:,0,:])\n",
    "            result,h,c = self.rnn(tf.expand_dims(cat,axis=1))\n",
    "            hidden = [h,c]\n",
    "            re_all.append(result)\n",
    "            \n",
    "        for i in range(1,cats.shape[1]):\n",
    "            step_cat = tf.expand_dims(cats[:,i,:],axis=1)\n",
    "            step_cat = self.mask(step_cat)\n",
    "            result,h,c = self.rnn(result,initial_state=hidden)\n",
    "            hidden = [h,c]\n",
    "            re_all.append(result)\n",
    "            \n",
    "        re_all = tf.concat(re_all,axis=1)\n",
    "        \n",
    "        return re_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_3d(matrix, vector):\n",
    "    ''' Computes cosine similarity of a given vector with vector rows from matrix'''\n",
    "\n",
    "    # normalize input\n",
    "    mat = tf.nn.l2_normalize(matrix,2)\n",
    "    mat2 = tf.nn.l2_normalize(tf.transpose(vector,[0,2,1]),1)\n",
    "\n",
    "    # multiply row i with row j using transpose\n",
    "    similar = tf.matmul(mat[:,:],mat2[:,:])\n",
    "\n",
    "    return similar\n",
    "\n",
    "def transfer_one_hot(sens):\n",
    "    one_hot_emb = []\n",
    "    for sen in sens:\n",
    "        tr_oh = np.expand_dims(one_hot_enc.transform(train_embs[0].reshape(-1,1)).toarray(),0)\n",
    "        one_hot_emb.append(tr_oh)\n",
    "    one_hot_sens = np.concatenate(one_hot_emb,axis=0)\n",
    "    \n",
    "    return one_hot_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder()\n",
    "dec = decoder()\n",
    "model = seq2(enc,dec)\n",
    "opt = tf.keras.optimizers.Adam(10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 94)\n",
      "(30, 94, 512)\n",
      "tf.Tensor(\n",
      "[[-7.6158482e-01  1.0411030e-14  2.7118299e-12 ... -1.9958385e-18\n",
      "  -3.1354685e-21  6.9254734e-06]\n",
      " [-7.6155180e-01  1.3594986e-13  3.4968838e-13 ... -3.3597533e-16\n",
      "  -1.2257518e-18 -8.4724779e-06]\n",
      " [-7.6102513e-01  1.1142798e-11  2.7721573e-12 ... -6.0346200e-13\n",
      "  -3.2247765e-17 -5.5418451e-07]\n",
      " ...\n",
      " [-7.6159406e-01  8.3390884e-13  1.0125324e-12 ... -1.4875412e-18\n",
      "  -1.4264918e-21 -7.6922124e-06]\n",
      " [-7.6145482e-01  4.6835615e-13  2.5929089e-11 ... -3.8370199e-15\n",
      "  -4.2875439e-19  1.3828137e-06]\n",
      " [-7.6157737e-01  1.0215803e-13  7.4864706e-12 ... -5.2996433e-15\n",
      "  -9.3295054e-19 -2.0693510e-06]], shape=(30, 256), dtype=float32)\n",
      "1\n",
      "['waiting', 'shake', 'move', 'dispatch', 'uncle', 'confined', 'sledge', 'posturing', 'shrieks', 'uncle', 'posturing', 'waiting', 'plays', 'distanced', 'posturing', 'shake', 'confined', 'move', 'uncle', 'posturing', 'interests', 'idea', 'enjoyed', 'alaska', 'confined', 'use', 'waiting', 'dowson', 'ebbed', 'rises']\n",
      "[['logical'], ['logical'], ['logical'], ['logical'], ['logical'], [\"lucy's\"], ['logical'], ['logical'], ['logical'], ['logical'], ['head'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['logical'], ['tiles'], ['logical'], ['logical'], ['head'], ['logical'], ['tiles']]\n",
      "2\n",
      "['intended', 'pulse', 'confined', 'reversed', 'orders', 'deeper', 'wrestled', 'confined', 'persisted', 'its', 'confined', 'crawfish', 'shake', 'confined', 'confined', 'symbols', 'surround', 'individuals', 'score', 'aeons', 'confined', 'plays', 'boiling', 'bustle', 'george', 'tenderness', 'sisters', 'dripping', 'brood', 'reality']\n",
      "[['physiognomy'], ['model'], ['johnston'], ['high'], ['logical'], ['enjoyed'], ['physiognomy'], ['logical'], ['johnston'], ['logical'], ['physiognomy'], ['sake'], ['high'], ['logical'], ['physiognomy'], ['logical'], ['logical'], ['model'], ['johnston'], ['high'], ['high'], ['logical'], ['johnston'], ['physiognomy'], ['johnston'], ['logical'], ['johnston'], ['physiognomy'], ['model'], ['johnston']]\n",
      "3\n",
      "['forenoon', 'paced', 'superior', 'union', 'fifteen', 'warfare', 'leaped', 'intimidated', 'confined', 'simplify', 'cliff', 'madge', 'reduced', 'moment', 'express', 'pampas', 'flushing', 'confined', 'plays', 'warfare', 'lovest', 'confined', 'enemy', 'inferiors', 'warfare', 'posturing', 'leading', 'confined', 'grounds', 'falling']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-385-3ac223f87efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_mfcc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_emb\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 '''loss = tf.reduce_mean(tf.math.sqrt(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-382-b634d79500ab>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, audio, txt_emb, txt_len, force_teaching)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_teaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0menc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mdec_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtxt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_teaching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdec_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-382-b634d79500ab>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, enc_out, embs, txt_len, force_teaching)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mstep_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-382-b634d79500ab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mstep_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-382-b634d79500ab>\u001b[0m in \u001b[0;36memb_to_word\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0memb_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcosine_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1614\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \"\"\"\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2229\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "df = display('start',display_id=True)\n",
    "\n",
    "with tf.device(f'GPU:{0}'):\n",
    "    for iter_num in range(1000):\n",
    "        for i in range(train_embs.shape[0]//30):\n",
    "            #batch_emb = transfer_one_hot(train_embs[i*30:(i+1)*30])\n",
    "            batch_emb = train_embs[i*30:(i+1)*30]\n",
    "            batch_mfcc = train_mfccs[i*30:(i+1)*30]\n",
    "            batch_len = train_len[i*30:(i+1)*30]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                result = model(batch_mfcc,batch_emb,batch_len,True)\n",
    "                loss=tf.reduce_mean(tf.reduce_sum(tf.math.sqrt(tf.reduce_sum(tf.math.pow(batch_emb-result,2),2)),1))\n",
    "                '''loss = tf.reduce_mean(tf.math.sqrt(\n",
    "                    tf.reduce_sum(tf.math.pow(cosine_similarity_3d(batch_emb,result),2),axis=1)))'''\n",
    "                train_loss(loss)\n",
    "                \n",
    "                #print(emb_to_word(tf.squeeze(result[0])))\n",
    "            grad = tape.gradient(loss,model.trainable_weights)\n",
    "            opt.apply_gradients(zip(grad,model.trainable_weights))\n",
    "            \n",
    "        print(train_loss.result().numpy())\n",
    "        df.update(tf.math.sqrt(tf.reduce_sum(tf.math.pow(batch_emb-result,2),2))[20])\n",
    "        print(emb_to_word(tf.squeeze(result[20])),emb_to_word(batch_emb[20]))\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(expdim(test_mfccs[1],0),sos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'neither', 'stab', 'stab', 'stab', 'stab', 'stab', 'stab', 'stab', 'stab', 'stab', 'stab', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later', 'later']\n"
     ]
    }
   ],
   "source": [
    "print(emb_to_word(tf.squeeze(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
