---


---

<h2 id="keras로-resnet-구조-작성하기">Keras로 resnet 구조 작성하기</h2>
<p>웹을 뒤적거리다 보면, Resnet의 모델 구조는 쉽게 튜토리얼을 구하여서 적용해볼 수 있다. 그 중 나는 Keras로 작성된 모델을 먼저 접했고,  resnet 50 구조를 15, 26 층의 구조로 바꾸어 보았었다.<br>
<a href="https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py">resnet50.py</a></p>
<p><a href="https://github.com/BaeKwangho/KWS/blob/master/models/resnet.py">내가 쓴 resnet</a><br>
( 참고, 특정 모델을 본따서 만든다고 feature 수를 조정해두었다.)</p>
<p>그리고 내가 적용하여야 하는 모델은 Sequential한 모델을 띄고 있었는데, 다음과 같았다.</p>
<pre><code>self.encoder = tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        tf.keras.layers.MaxPool2D((2, 2)),

        tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        tf.keras.layers.MaxPool2D((2, 2)),

        tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        tf.keras.layers.MaxPool2D((2, 2)),

        tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.ReLU(),
        tf.keras.layers.MaxPool2D((2, 2)), Flatten()]
    )
</code></pre>
<p>그리고, 이를 담는 클래스는 keras.model을 상속받지만 compile 혹은 build 없이 manual한 학습을 진행하고 있었다. 여태까지 resnet의 model 모듈에 의해 쉽게 학습을 하던 나에게는 어렵게 느껴질 수 밖에 없었다…</p>
<p>Sequential layer는 말그대로 이어지는 레이어들의 집합이라, residual한 resnet을 구현할 수 없다고 판단했다. (검색했지만… 검색능력의 한계였을지도 ㅠ)<br>
x_shortcut을 더해주어야하는데 어떻게 하겠는가…</p>
<h3 id="해결방법">해결방법</h3>
<p>사실 학부생인 내가 삽질한 것이 맞을 지는 모르겠지만(검토를 받을 예정이다), 일단은 오늘의 삽질을 올리고 싶어서 적어본다 ㅎㅎ</p>
<p>우선 나는 Sequential 객체가 모델의 전부가 아니라는 것에 집중했다. Class의 call함수 속에서 forward 를 진행하고, 그 결과값만 loss로 저장을 한 후 return 해주는 구조였다.</p>
<pre><code>def train_step(loss_func, support, query):
    # Forward &amp; update gradients
    with tf.GradientTape() as tape:
        loss, acc = model(support, query)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(
        zip(gradients, model.trainable_variables))
    print(model.summary())

    # Log loss and accuracy for step
    train_loss(loss)
    train_acc(acc)
</code></pre>
<p>tf.GradientTape.gradient와 optimizer.apply_gradients는 return 받은 loss를 backprop하여 모델에 저장해주고,  loss 평균을 계산해준다.</p>
<p>그러면 Sequential 객체를 대신해서 model의 함수가 들어가면 되지 않는가? 어차피 두 모델 모두 trainable_parameter를 갖고, loss 계산 값만을 사용하여 model 수준에서의 자체적인 backprop을 안하니까? 뭐 괜찮을 거라고 생각했다.</p>
<p>그래서 다음과 같은 구조의 resnet을 만들어 끼우는데 성공했다.</p>
<pre><code>def block(input_tensor,num_filter,num_dilation):

    x_shortcut = input_tensor
    #step 1
    x = Conv2D(filters=num_filter, kernel_size=(9,1), padding='same',
                              dilation_rate=(num_dilation,1),strides=1)(x_shortcut)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    print(x.shape)
    #step 2
    x = Conv2D(filters=num_filter, kernel_size=(9,1), padding='same',
                              dilation_rate=(num_dilation,1),strides=1)(x_shortcut)
    x = tf.keras.layers.BatchNormalization()(x)

    #skip connection
    x_shortcut = Conv2D(filters=num_filter, kernel_size=(1,1), padding='same',
                              dilation_rate=(num_dilation,1),strides=1)(x_shortcut)
    x_shortcut = tf.keras.layers.BatchNormalization()(x_shortcut)
    x_shortcut = tf.keras.layers.ReLU()(x_shortcut)

    #addition
    x = Add()([x, x_shortcut])
    x = tf.keras.layers.ReLU()(x)

    return x

def Resnet(input_shape,filters=[16,24,32,48],dilations=[1,2,4]):

    x_input = Input(input_shape)
    print('input',x_input.shape)
    filter1,filter2,filter3,filter4 = filters
    dilation1, dilation2, dilation3 = dilations

    #conv 3*1
    x = Conv2D(filters=filter1, kernel_size=(3,1), padding='valid')(x_input)
    print(x.shape)
    #block 1
    x = block(x,filter2,dilation1)
    print(x.shape)
    #blcok 2
    x = block(x,filter3,dilation2)
    print(x.shape)
    #block 3
    x = block(x,filter4,dilation3)
    print(x.shape)
    x = AveragePooling2D((2,1))(x)
    print(x.shape)

    x = Flatten()(x)

    print(x.shape)
    model = Model(inputs=x_input,outputs=x,name='ProtoRes')

    return model
</code></pre>
<p>뭐 일단은 돌아가니까 괜찮다고 생각한다만, 성능 검증과 Propose를 바꾸었다는 부분에서 유효성 검증이 추가로 필요하며, 그것을 진행하고자 한다.</p>

