{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week4\n",
    "#### 그냥 인터넷에서 본 이것저것\n",
    "\n",
    "tensor 2 부터 지원되는 tf.gradients는 graph로 표현된 층으로부터의 미분값들을 구한다.<br>\n",
    "궁금해서 tf.graph를 찾아봤는데, 이는 파이썬 내에서만의 operation으로 구성된 기존 레이어들에서 벗어나,\n",
    "graph 형식의 data structure를 구현하여 파이썬 interpreter가 없는 embeded, mobile, server 등에서 tf를 활용할 수 있도록 한다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하긴\n",
    "현재 보고 있는 강의도 그렇다. 기본적인 원리를 설명하기 위해 numpy로 직접 하나하나 구현하는 operation이지만, 이것은 딥러닝 프레임워크가 나옴에 따라 구조화 되어 우리가 편히 사용하는 것이니까. <br>\n",
    "우선적으로 원리를 알기 위해 이 강의를 보고 있음을 잊지 말자.\n",
    "----\n",
    "hidden layer의 weight를 그림으로 나타내면, 서로 다른 빗면으로 나타나는 그림 여러장을 구할 수 있는데, 이는 DNN에서 주어진 iuput image에서 각 빗면이 어디에 위치하는지를 구하는 것이라고 한다.(아마 특이한 케이스일 것이다. 이미 다 세팅된..)<br>\n",
    "\n",
    "그리고 layer를 지날때마다 x의 사이즈가 줄어들면서 눈, 코, 입 단위의 프레임이 턱, 이마, 안면 등으로 확대되며 학습이 진행된다. <br>\n",
    "즉, 작은 부분을 찾아서 그들을 합쳐가며 복잡한 부분을 찾을 수 있는 것이라고..\n",
    "\n",
    "마지막에, 작은 모델부터 시작해서 (logistic regression) 문제를 해결할 수 있는 모델을 찾는 것이 효율적이라고 하셨다.<br>\n",
    "(물론, 문제에 따라 접근해야겟지만..) 이런 응용력이 기반되어야 할듯하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loss가 구해짐에 따라**\n",
    "\n",
    "backpropagation이 가능해지며 역으로 전파하고, 현재 loss 공식에 맞는 derivative term이 구해지고 전파되며 derivative weight가 구해짐\n",
    "\n",
    "어쨌든.. 오늘은 DNN의 backpropagation을 배웠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as Layer\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "x = open('/root/storage/DATA/pickle/checked','rb')\n",
    "data = pickle.load(x)\n",
    "x.close()\n",
    "\n",
    "cycle = np.random.permutation(25)\n",
    "exp_data = data.reshape(-1,data.shape[1],data.shape[-1]*data.shape[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(shape):\n",
    "    x_input = Layer.Input(shape)\n",
    "    \n",
    "    x = Layer.Dense(100,activation='tanh')(x_input)\n",
    "    x = Layer.Dense(50,activation='tanh')(x)\n",
    "    x = Layer.Dense(25,activation='tanh')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input,outputs=x,name='custom')\n",
    "    return model\n",
    "#x = np.random.randn(7,7760)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일단 너무 추우니까 철수 ;\n",
    "\n",
    "---\n",
    "\n",
    "학습을 하기 위해 loss를 구하는 식을 짜야하는데,, 우선 대상을 정하고 그 사이에서 식을 정립해야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7760,)\n",
      "WARNING:tensorflow:Layer dense_377 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "iter : 0 tf.Tensor(13.665147, shape=(), dtype=float32)\n",
      "iter : 1 tf.Tensor(13.699387, shape=(), dtype=float32)\n",
      "iter : 2 tf.Tensor(13.916614, shape=(), dtype=float32)\n",
      "iter : 3 tf.Tensor(14.426247, shape=(), dtype=float32)\n",
      "iter : 4 tf.Tensor(13.777717, shape=(), dtype=float32)\n",
      "iter : 5 tf.Tensor(13.799385, shape=(), dtype=float32)\n",
      "iter : 6 tf.Tensor(14.059562, shape=(), dtype=float32)\n",
      "iter : 7 tf.Tensor(13.602678, shape=(), dtype=float32)\n",
      "iter : 8 tf.Tensor(13.630167, shape=(), dtype=float32)\n",
      "iter : 9 tf.Tensor(13.617482, shape=(), dtype=float32)\n",
      "iter : 10 tf.Tensor(13.760843, shape=(), dtype=float32)\n",
      "iter : 11 tf.Tensor(13.942029, shape=(), dtype=float32)\n",
      "iter : 12 tf.Tensor(13.707945, shape=(), dtype=float32)\n",
      "iter : 13 tf.Tensor(13.661324, shape=(), dtype=float32)\n",
      "iter : 14 tf.Tensor(13.676248, shape=(), dtype=float32)\n",
      "iter : 15 tf.Tensor(13.698314, shape=(), dtype=float32)\n",
      "iter : 16 tf.Tensor(13.738699, shape=(), dtype=float32)\n",
      "iter : 17 tf.Tensor(13.6628, shape=(), dtype=float32)\n",
      "iter : 18 tf.Tensor(13.617965, shape=(), dtype=float32)\n",
      "iter : 19 tf.Tensor(13.642742, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "exp_shape = exp_data[0,0].shape\n",
    "print(exp_shape)\n",
    "model = build(exp_shape)\n",
    "def calc_euclidian_dists(x, y):\n",
    "    \"\"\"\n",
    "    Calculate euclidian distance between two 3D tensors.\n",
    "    Args:\n",
    "        x (tf.Tensor):\n",
    "        y (tf.Tensor):\n",
    "    Returns (tf.Tensor): 2-dim tensor with distances.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 1)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(10e-2)\n",
    "iteration = 20\n",
    "iper_cls = 5\n",
    "with tf.device('/CPU:0'):\n",
    "    for i in range(iteration):\n",
    "        permu = np.random.permutation(25)[:iper_cls]\n",
    "        data_pool = exp_data[permu].reshape(iper_cls*exp_data.shape[1],exp_data.shape[-1])\n",
    "        #x_data = data_pool[:,:20]\n",
    "        #y_data = data_pool[:,20:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            result = model(data_pool)\n",
    "            x_result = result[:100]\n",
    "            y_result = result[100:]\n",
    "            \n",
    "            y = np.tile(np.arange(5)[:, np.newaxis], (1, 180))\n",
    "            y_onehot = tf.cast(tf.expand_dims(y,axis=2), tf.float32)\n",
    "            \n",
    "            x_result=tf.tile(x_result,(y_result.shape[0]//x_result.shape[0],1))\n",
    "            \n",
    "            afc = calc_euclidian_dists(x_result,y_result)\n",
    "            \n",
    "            log_p_y = tf.nn.log_softmax(-afc, axis=-1)\n",
    "            log_p_y = tf.reshape(log_p_y, [iper_cls, 180, -1])\n",
    "            \n",
    "            loss = -tf.reduce_mean(tf.reshape(tf.reduce_sum(tf.multiply(y_onehot, log_p_y), axis=-1), [-1]))\n",
    "        #print(model.trainable_weights)\n",
    "        gradients = tape.gradient(loss,model.trainable_weights)\n",
    "        print('iter :',i, loss)\n",
    "        #print('grad : ',gradients)\n",
    "        #print('weight : ',model.trainable_weights)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
